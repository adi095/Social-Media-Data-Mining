{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6a32de-d4b8-4270-9a73-e29b431e6fe5",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "\n",
    "##### Adishree Sane, 702207239, adsane@syr.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5131b-204c-42f0-bcd4-4613574e6f47",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6368b0-2687-4ce0-9bfc-a1843bc6c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8385f4-3d9c-4881-9a56-aa7e50159dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adiun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adiun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adiun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f721af02-21b2-4fc6-b2f7-205309100252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class                                               text\n",
      "0   Pos  Now all @Apple has to do is get swype on the i...\n",
      "1   Pos  @Apple will be adding more carrier support to ...\n",
      "2   Pos  Hilarious @youtube video - guy does a duet wit...\n",
      "3   Pos  @RIM you made it too easy for me to switch to ...\n",
      "4   Pos  I just realized that the reason I got into twi...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('twitter-sanders-apple2.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f09d828-80d5-40e3-bbbd-ce1843bba3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "  \n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "  \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    words = [word for word in words if len(word) > 2 and len(word) < 20]\n",
    "    \n",
    "\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e687dd48-35b3-4905-8512-7e1ffa715088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\adiun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad0c522-632e-43ed-b646-074b72bdb2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Now all @Apple has to do is get swype on the i...   \n",
      "1  @Apple will be adding more carrier support to ...   \n",
      "2  Hilarious @youtube video - guy does a duet wit...   \n",
      "3  @RIM you made it too easy for me to switch to ...   \n",
      "4  I just realized that the reason I got into twi...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                   appl get swype iphon crack iphon  \n",
      "1              appl ad carrier support iphon announc  \n",
      "2  hilari youtub video guy duet appl siri pretti ...  \n",
      "3                rim made easi switch appl iphon see  \n",
      "4          realiz reason got twitter ios5 thank appl  \n"
     ]
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "print(data[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451ff770-381f-4eae-853d-5cce9094de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer_unigrams = CountVectorizer(binary=True, min_df=3, stop_words='english')\n",
    "X = vectorizer_unigrams.fit_transform(data['cleaned_text'])\n",
    "y = data['class'].apply(lambda x: 1 if x == 'Pos' else 0)  # Convert 'Pos' and 'Neg' to 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3356125-c242-41d2-8f27-3b7a840e801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (479, 312)\n",
      "Top 10 Unigram Features: ['3g' 'absolut' 'actual' 'air' 'album' 'alreadi' 'alway' 'amaz' 'android'\n",
      " 'annoy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Feature Matrix Shape:\", X.shape)\n",
    "\n",
    "print(\"Top 10 Unigram Features:\", vectorizer_unigrams.get_feature_names_out()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ae4ee8-76a7-47fd-89e3-e82d09f6a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3de2edf-947d-4e4a-aaf7-bdc6a31ddf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['class', 'text', 'cleaned_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94560f5-4f25-4d68-a3c1-a37315a37cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84        61\n",
      "           1       0.70      0.80      0.75        35\n",
      "\n",
      "    accuracy                           0.80        96\n",
      "   macro avg       0.79      0.80      0.79        96\n",
      "weighted avg       0.81      0.80      0.80        96\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        61\n",
      "           1       0.67      0.63      0.65        35\n",
      "\n",
      "    accuracy                           0.75        96\n",
      "   macro avg       0.73      0.72      0.73        96\n",
      "weighted avg       0.75      0.75      0.75        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(\"Naïve Bayes Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08eab4a-bc22-4f97-bc52-6d304f1eb719",
   "metadata": {},
   "source": [
    "### Exeriment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ba0d45-69d9-4bac-afce-40e6c505f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\adiun\\anaconda3\\envs\\cudasupport\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.8/12.2 MB 35.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 36.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 29.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 54.4 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.1 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.1-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 29.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 5.2/5.4 MB 39.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.11.1 pydantic-core-2.33.0 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 typing-inspection-0.4.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8c215b-f6c2-483d-9c3f-3ac85f7b1d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 19.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 33.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 32.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b27f0fd7-9ca8-4384-9ad8-fddcc2a1a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87bdfed9-fe9f-44df-ae90-e9122a7c4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pos_tagging_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    # Create POS tag representation \"word_POS\"\n",
    "    return \" \".join([f\"{token.text}_{token.pos_}\" for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6faa11d-40b3-4034-bdf3-7dbee02a8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57da73c2-35f0-4603-ba30-4108f88d4e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Now all @Apple has to do is get swype on the i...   \n",
      "1  @Apple will be adding more carrier support to ...   \n",
      "2  Hilarious @youtube video - guy does a duet wit...   \n",
      "3  @RIM you made it too easy for me to switch to ...   \n",
      "4  I just realized that the reason I got into twi...   \n",
      "\n",
      "                                      pos_text_spacy  \n",
      "0  appl_ADV get_VERB swype_NOUN iphon_NOUN crack_...  \n",
      "1  appl_VERB ad_NOUN carrier_NOUN support_PROPN i...  \n",
      "2  hilari_PROPN youtub_PROPN video_NOUN guy_NOUN ...  \n",
      "3  rim_NOUN made_VERB easi_ADJ switch_NOUN appl_P...  \n",
      "4  realiz_NOUN reason_NOUN got_VERB twitter_ADJ i...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['pos_text_spacy'] = data['cleaned_text'].apply(pos_tagging_spacy)\n",
    "print(data[['text', 'pos_text_spacy']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a4eb2b2-ef9f-4c79-94df-89e47c04cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pos = CountVectorizer(binary=True, stop_words='english')\n",
    "X_pos = vectorizer_pos.fit_transform(data['pos_text_spacy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7849932-606c-434c-8456-4a7226e6a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Feature Matrix Shape: (479, 1878)\n",
      "Top 10 POS Features: ['100_num' '135_num' '199_num' '1hr_propn' '1st_adj' '1st_propn'\n",
      " '1stgen_num' '2001_num' '200_num' '2012_num']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"POS Feature Matrix Shape:\", X_pos.shape)\n",
    "\n",
    "print(\"Top 10 POS Features:\", vectorizer_pos.get_feature_names_out()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a4534ee-de22-4f88-a42e-c2ca7b25edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Classification Report (POS):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.84        61\n",
      "           1       0.70      0.86      0.77        35\n",
      "\n",
      "    accuracy                           0.81        96\n",
      "   macro avg       0.80      0.82      0.81        96\n",
      "weighted avg       0.83      0.81      0.82        96\n",
      "\n",
      "SVM Classification Report (POS):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83        61\n",
      "           1       0.71      0.69      0.70        35\n",
      "\n",
      "    accuracy                           0.78        96\n",
      "   macro avg       0.76      0.76      0.76        96\n",
      "weighted avg       0.78      0.78      0.78        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(X_pos, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_model_pos = MultinomialNB()\n",
    "nb_model_pos.fit(X_train_pos, y_train_pos)\n",
    "\n",
    "svm_model_pos = SVC(kernel='linear')\n",
    "svm_model_pos.fit(X_train_pos, y_train_pos)\n",
    "\n",
    "y_pred_nb_pos = nb_model_pos.predict(X_test_pos)\n",
    "print(\"Naïve Bayes Classification Report (POS):\")\n",
    "print(classification_report(y_test_pos, y_pred_nb_pos))\n",
    "\n",
    "y_pred_svm_pos = svm_model_pos.predict(X_test_pos)\n",
    "print(\"SVM Classification Report (POS):\")\n",
    "print(classification_report(y_test_pos, y_pred_svm_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f2f91-d52c-4356-b27d-13b443d792d9",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac0d08-d292-4c38-bed1-336950b85b59",
   "metadata": {},
   "source": [
    "#### Naïve Bayes (Unigrams vs POS features):\n",
    "##### Unigrams:\n",
    "\n",
    "Negative Class: Precision = 0.88, Recall = 0.80, F1-Score = 0.84\n",
    "\n",
    "Positive Class: Precision = 0.70, Recall = 0.80, F1-Score = 0.75\n",
    "\n",
    "Accuracy: 0.80\n",
    "\n",
    "##### POS Features:\n",
    "\n",
    "Negative Class: Precision = 0.91, Recall = 0.79, F1-Score = 0.84\n",
    "\n",
    "Positive Class: Precision = 0.70, Recall = 0.86, F1-Score = 0.77\n",
    "\n",
    "Accuracy: 0.81\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "POS features improve Naïve Bayes performance for negative and positive sentiment, with an increase in recall for positive sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d304-ed60-4d25-9d8d-ceb9dfdd1963",
   "metadata": {},
   "source": [
    "#### SVM (Unigrams vs POS features):\n",
    "##### Unigrams:\n",
    "\n",
    "Negative Class: Precision = 0.79, Recall = 0.82, F1-Score = 0.81\n",
    "\n",
    "Positive Class: Precision = 0.67, Recall = 0.63, F1-Score = 0.65\n",
    "\n",
    "Accuracy: 0.75\n",
    "\n",
    "##### POS Features:\n",
    "\n",
    "Negative Class: Precision = 0.82, Recall = 0.84, F1-Score = 0.83\n",
    "\n",
    "Positive Class: Precision = 0.71, Recall = 0.69, F1-Score = 0.70\n",
    "\n",
    "Accuracy: 0.78\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "POS features improve SVM performance for negative sentiment, but the increase for positive sentiment is average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871d8bd-1c73-4854-8936-ba75ec9825f4",
   "metadata": {},
   "source": [
    "#### Overall Observations:\n",
    "Naïve Bayes benefits more from POS features than SVM regarding accuracy and recall for positive sentiment.\n",
    "\n",
    "SVM shows improvement with POS features in negative sentiment and overall accuracy.\n",
    "\n",
    "POS features improve precision and recall for negative sentiment in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d2a56-0acc-4db3-9697-19724766b030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
